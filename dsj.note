python
	python #进入python交互窗口，python注释已#开头
	print('hello,world!')
	exit()
	
	#!/usr/bin/env python3 #在linux，文件头加入这一行，就成为可执行文件
	print('The quick brown fox', 'jumps over', 'the lazy dog') 
	
	#python字符串可以用单引号，也可以用双引号。单引号和双引号是等同的。反斜杠\可以转义，r''单引号中的内容会屏蔽转义
	#布尔值True和False，布尔值计算 and or not(注意和java不一样)
	#空值None
	#运算，python中独有的板除//：8//3=2
	
	#编码
	print('中文'.encode('utf-8')) result is: b'\xe4\xb8\xad\xe6\x96\x87' #python3中，同理 print(b'\xe4\xb8\xad\xe6\x96\x87'.decode('UTF-8'))#中文  
	
	#函数,默认参数 可变参数 递归
	1.def my_abs(x):
    if x>=0:
	    return x
    else:
        return -x
	
	2.def fact(n):
    if n==1:
        return 1
    return n * fact(n - 1)
	
	#包、模块
	目录下面置_init_.py文件，就声明这个目录是个包。包下面有xy.py，就声明xy是该包下面的一个模块
	
	#安装和使用第三方模块 mysql
	pip install mysql-connector-python #安装mysql connector
	import mysql.connector
	#mysql connection config item
	'''dict Found at: mysql.connector.constants
	DEFAULT_CONFIGURATION = {
    'database':None, 
    'user':'', 
    'password':'', 
    'host':'127.0.0.1', 
    'port':3306, 
    'unix_socket':None, 
    'use_unicode':True, 
    'charset':'utf8mb4'...}'''
	conn = mysql.connector.connect(user='root', password='liyuff', database='mysql')
	cursor = conn.cursor()
	cursor.execute('select * from user where user = %s', ('root',))
	values = cursor.fetchall() #value list
	conn.close()
	
	#类
	获取对象的信息
	type() 
	#h = Dog()
	isinstance(h, Animal)
	dir()
	getattr() setattr()以及hasattr()
	使用__slots__限制类的属性
	使用@property将方法设置为属性，供外部访问
	定制类
	元类
hadoop 
	/root/dev/hadoop-2.7.6/bin/hdfs dfs -du -h README.md //查看文件大小
	/root/dev/hadoop-2.7.6/bin/hdfs dfs -rmr README //删除文件夹
spark
	download spark in http://spark.apache.org/
	tar -xf spark*.tar
	bin/spark-shell
	in shell>
		spark.read.textFile("README.md").count() //return 205
		sc.parallelize(List("hello,how are you baby, you are right!")).flatMap(_.split(" ")).map(el=>(el,1)).reduceByKey((a,b)=>a+b).collect().foreach(el=>println(el))
apark-config
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/slaves
	single
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/spark-env.sh
	export JAVA_HOME=/usr/local/jdk
	# 一般来说，spark任务有很大可能性需要去HDFS上读取文件，所以配置上
	# 如果说你的spark就读取本地文件，也不需要yarn管理，不用配
	export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.6/etc/hadoop

	# 设置Master的主机名
	export SPARK_MASTER_HOST=single
	# 提交Application的端口，默认就是这个，万一要改呢，改这里
	export SPARK_MASTER_PORT=7077
	# 每一个Worker最多可以使用的cpu core的个数，我虚拟机就一个...
	# 真实服务器如果有32个，你可以设置为32个
	export SPARK_WORKER_CORES=1
	# 每一个Worker最多可以使用的内存，我的虚拟机就2g
	# 真实服务器如果有128G，你可以设置为100G
	export SPARK_WORKER_MEMORY=1g

spark-sql
	bin/spark-shell bin/spark-shell --jars /usr/local/mysql/mysql-test/jdbc/mysql-connector-java-5.1.44.jar 
	val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/mysql").option("dbtable", "user").option("user", "root").option("password", "liyuff").load()
	#jdbcDF.show()
	jdbcDF.registerTempTable("test")
	jdbcDF.sqlContext.sql("select user from test").collect.foreach(println)
	jdbcDF.toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	jdbcDF.sqlContext.sql("select user,host from test").toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	
	jdbcDF.toJavaRDD.saveAsTextFile("sparkuser") //写入hdfs文件至sparkuser/part-00000，当然spark连接的是hadoop
sqoop
	bin/sqoop list-tables --username root --password 'liyuff' --connect jdbc:mysql://localhost:3306/mysql?characterEncoding=UTF-8
	bin/sqoop create-hive-table --connect jdbc:mysql://localhost:3306/sqoop_hive?characterEncoding=UTF-8 --table test --username root -password 'liyuff' --hive-database sqoop_hive
	bin/sqoop import --connect jdbc:mysql://localhost:3306/sqoop_hive?characterEncoding=UTF-8 --table test --username root -password  'liyuff' --fields-terminated-by ',' --hive-import --hive-database sqoop_hive  -m  1
	
hive
	启动hiveserver2：./dev/apache-hive-1.2.2-bin/bin/hive --service hiveserver2
	启动：./dev/apache-hive-1.2.2-bin/bin/beeline
	beeline连接：!connect jdbc:hive2://<host>:<port>/<db>
	

	

