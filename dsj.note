hadoop 
	单节点安装：
	set hostname
	/usr/hadoop/hadoop2.8.3/etc/hadoop/slaves
	
	set JAVA_HOME
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hadoop-env.sh
	
	set PORT and TMPDIR
	/usr/hadoop/hadoop2.8.3/etc/hadoop/core-site.xml 
	<configuration>
		<!-- 指定HDFS老大（namenode）的通信地址 -->
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://127.0.0.1:9000</value>
		</property>
		<!-- 指定hadoop运行时产生文件的存储路径 -->
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/root/datahadoop/tmp</value>
		</property>
	</configuration>

	
	
	set namenode and datanode dir
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hdfs-site.xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>/root/datahadoop/name</value>
			<description>namenode上存储hdfs名字的物理存储位置 </description>
		</property>
		<property>
			<name>dfs.data.dir</name>
			<value>/root/datahadoop/data</value>
			<description>datanode上数据块的物理存储位置</description>
		</property>
		<!-- 设置hdfs副本数量,默认为3,设置为1 这样每个block只会存在一份 -->
		<property>
			<name>dfs.replication</name>
			<value>1</value>
		</property>
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
			<description>对hdfs上的文件进行读写时,是否检查权限</description>
		</property>
	</configuration>

	set no password login
	ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	chmod 0600 ~/.ssh/authorized_keys
	format namenode
	bin/hdfs namenode -format
	startup dfs 
	sbin/start-dfs.sh 
	browser with: http://192.168.2.5:50070
	create fileholder in hdfs 
	/bin/hdfs dfs -mkdir /test 
	browser get the hileholder
	stop hdfs
	/sbin/stop-dfs.sh
	set yarn
	mapred-site.xml
	<configuration>
		<!-- 启用yarn作为资源管理框架 -->
		<property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		</property>
	</configuration>
	
	set mapreduce_shuffle
	yarn-site.xml
	<configuration>
		<!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 -->
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
	</configuration>

	startup yarn 
	/sbin/start-yarn.sh
	browser with: http://192.168.2.5:8088
	stop yarn
	/sbin/stop-yarn.sh
	
	设置jvm内存：
	set 'export HADOOP_HEAPSIZE=2000' in $HADOOP_HOME/etc/hadoop/hadoop-env.sh
	set in $HADOOP_HOME/etc/hadoop/mapred-site.xml
	<property>
		<name>mapred.child.java.opts</name>
		<value>-Xmx2000m</value>
	</property>

	/root/dev/hadoop-2.7.6/bin/hdfs dfs -du -h README.md //查看文件大小
	/root/dev/hadoop-2.7.6/bin/hdfs dfs -rmr README //删除文件夹

hive-1.1.0（基于hadoop 2.6）
	修改conf/hive-env.xml中的HADOOP_HOME，HIVE_CONF_DIR
	修改conf/hive-site.xml(conf/hive-default.xml.tmplate)中的：
	<configuration>
		<!--一般启动报错，需要加入下面2property-->
		<property>
			<name>system:java.io.tmpdir</name>
			<value>/root/hive/tmpdir</value>
		</property>
		<property>
			<name>system:user.name</name>
			<value>hive</value>
		</property>
		
		<property>
			<name>javax.jdo.option.ConnectionURL</name>
			<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNoExist=true&amp;useSSL=false</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionDriverName</name>
			<value>com.mysql.jdbc.Driver</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionUserName</name>
			<value>hive</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionPassword</name>
			<value>hive</value>
		</property>
	</configuration>
	
	初始化
	$HIVE_HOME/schematool -dbType mysql -initSchema
	
	启动hive
	$HIVE_HOME/hive

	hql:
	CREATE TABLE pokes (foo INT, bar STRING);
	insert into pokes values(3,'三');
	insert into pokes values(4,'四');//1.1.0不支持
	DESCRIBE pokes;
	
	CREATE TABLE `stu3`(
	  `id` int,
	  `name` string,
	  `age` int)
	ROW FORMAT DELIMITED
	  FIELDS TERMINATED BY ',';
	  
	load data local inpath '/root/t' overwrite into table stu3;
	
	t文件内容:
	3,往来,5
	4,来往,5
	
	csv文件（下面这条才能正确加载csv文件,文件中有逗号，用双引号包裹，双引号需要转义）
	create table csv_t1(a string,b string,c string) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	t文件内容
	3,往来,5
	4,abc,5
	6,'abc',7
	9,"你好,世界",9
	
	create table csv_t2(a string,b string,c string) PARTITIONED BY (ds STRING) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	load data local inpath '/root/t' overwrite into table csv_t2 PARTITION (ds='2008-08-18');--文件内容还是上面的一样
	SELECT a.foo FROM csv_t2 a WHERE a.ds='2008-08-15';
	hdfs中目录结构：
	/user/hive/warehouse/csv_t2/ds=2008-08-15/t
	/user/hive/warehouse/csv_t2/ds=2008-08-18/t
	
	使用spark读取hive数据：
	概述，首先要启动hadoop，hive metastore，spark，必要是还需要mysql的驱动jar放入spark的jars目录下
	1，拷贝hive-site.xml到spark/conf
	2，开启hive元数据服务：hive  --service metastore
	3，开启hadoop服务：sh  $HADOOP_HOME/sbin/start-all.sh
	4.开启spark服务：sh $SPARK_HOME/sbin/start-all.sh
	5.进入spark-shell：spark-shell
		val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
		sqlContext.sql("SELECT * FROM csv_t2 where ds='2008-08-15'").collect().foreach(println)
	6.退出:quit
	
	启动hibeserver2和beeline //1.1.0不支持
	$HIVE_HOME/bin/hiveserver2
	$HIVE_HOME/bin/beeline -u jdbc:hive2://
hbase-1.2.0(基于hadoop-2.6)
	1，download tar -xf cd hbase
	2，edit conf/hbase-env.sh JAVA_HOME
	3. edit conf/hbase-site.xml
	<configuration>
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://localhost:9000/hbase</value>
        </property>
	</configuration>
	4. bin/start-hbase.sh
	5. 进入shell
		bin/hbase shell
		
		create 'test', 'cf'
		--添加数据
		put 'test', 'row1', 'cf:a', '大数据'
		--查询列簇
		get 'test','row1','cf'
		--查询列
		get 'test','row1','cf:a'
		--扫描列
		scan 'test',{COLUMN=>'cf'} or scan 'test',{COLUMNS=>'cf:a'}
		--增加列簇
		alter 'test', 'id'
		--删除列簇
		alter 'test', {NAME => 'id', METHOD => 'delete’}
		--删除列
		delete 'test','row1','cf:a'
		--删除行
		deleteall 'test','row1'
		--全表扫描
		scan 'test'
		//如果出现乱码，在python中：print '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')查看
		
		--禁用和删除表
		disable 'test'
		drop 'test'
	6. 退出shell :quit
spark
	download spark in http://spark.apache.org/
	tar -xf spark*.tar
	bin/spark-shell
	in shell>
		spark.read.textFile("README.md").count() //return 205
		sc.parallelize(List("hello,how are you baby, you are right!")).flatMap(_.split(" ")).map(el=>(el,1)).reduceByKey((a,b)=>a+b).collect().foreach(el=>println(el))
		
	1，举例说明map，flatMap以及reduce，reduceByKey的用法。
	 sc.textFile("text").flatMap(s=>s.split(" ")).map(e=>(e,1)).reduceByKey((a,b)=>a+b).collect()
	 sc.parallelize(Array(1,2,3,4,5)).reduce((a,b)=>a+b)
	map是匹配的意思，flatMap是分解的意思，reduce是计算的意思，reduceByKey是按key计算的意思

	2，sortBy和sortByKey的用法
	  sc.parallelize(Array(3,1,5,2,9)).sortBy(e=>e).collect()
	  --the preline 也可以写成 sc.parallelize(Array(3,1,5,2,9)).sortBy(identity).collect()
	  sc.parallelize(Array((1,2),(4,0),(1,3))).sortBy(a=>a._2).collect() 
	  sc.parallelize(Array((1, 6, 3), (2, 3, 3), (1, 1, 2), (1, 3, 5), (2, 1, 2))).sortBy(e=>(e._1,e._2)).collect()
	sortBy是rdd的排序，输入参数是指定的排序字段或元组

	3，filter的用法
	  sc.parallelize(Array(1,2,3,4,5,6,7,8)).filter(e=>e%2!=0).collect

	4，spark读取文件textFile，输出文件saveAsTextFile([String])
apark-config
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/slaves
	single
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/spark-env.sh
	export JAVA_HOME=/usr/local/jdk
	# 一般来说，spark任务有很大可能性需要去HDFS上读取文件，所以配置上
	# 如果说你的spark就读取本地文件，也不需要yarn管理，不用配
	export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.6/etc/hadoop

	# 设置Master的主机名
	export SPARK_MASTER_HOST=single
	# 提交Application的端口，默认就是这个，万一要改呢，改这里
	export SPARK_MASTER_PORT=7077
	# 每一个Worker最多可以使用的cpu core的个数，我虚拟机就一个...
	# 真实服务器如果有32个，你可以设置为32个
	export SPARK_WORKER_CORES=1
	# 每一个Worker最多可以使用的内存，我的虚拟机就2g
	# 真实服务器如果有128G，你可以设置为100G
	export SPARK_WORKER_MEMORY=1g

spark-sql
	bin/spark-shell bin/spark-shell --jars /usr/local/mysql/mysql-test/jdbc/mysql-connector-java-5.1.44.jar 
	val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/mysql").option("dbtable", "user").option("user", "root").option("password", "liyuff").load()
	#jdbcDF.show()
	jdbcDF.registerTempTable("test")
	jdbcDF.sqlContext.sql("select user from test").collect.foreach(println)
	jdbcDF.toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	jdbcDF.sqlContext.sql("select user,host from test").toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	
	jdbcDF.toJavaRDD.saveAsTextFile("sparkuser") //写入hdfs文件至sparkuser/part-00000，当然spark连接的是hadoop
	
	#json格式
	var df = spark.read.json("person.json")
	df.printSchema
	df.registerTempTable("p")
	spark.sql("select * from p").show()
	spark.sql("select * from p").collect().foreach(print)
	
	联合查询：
	persion.json
	{"id":1,"name":"Tom","age":25}
	"id":2,"name":"LiLi","age":37}
	{"id":3,"name":"Han","age":42}
	addr.json
	{"id":1,"name":"中国","pid":1}
	{"id":2,"name":"美国","pid":1}
	{"id":3,"name":"日本","pid":4}

	spark.read.json("person.json").registerTempTable("p")
	spark.read.json("addr.json").registerTempTable("a")
	spark.sql("select * from p join a on p.id=a.pid").show()
	spark.sql("select * from p left join a on p.id=a.pid").show()
	spark.sql("select * from p right join a on p.id=a.pid").show()
	spark.sql("select * from p full outer join a on p.id=a.pid").show()
	:quit

