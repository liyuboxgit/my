spark
	download spark in http://spark.apache.org/
	tar -xf spark*.tar
	bin/spark-shell
	in shell>
		spark.read.textFile("README.md").count() //return 205
apark-config
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/slaves
	single
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/spark-env.sh
	export JAVA_HOME=/usr/local/jdk
	# ä¸€èˆ¬æ¥è¯´ï¼Œsparkä»»åŠ¡æœ‰å¾ˆå¤§å¯èƒ½æ€§éœ€è¦å»HDFSä¸Šè¯»å–æ–‡ä»¶ï¼Œæ‰€ä»¥é…ç½®ä¸Š
	# å¦‚æœè¯´ä½ çš„sparkå°±è¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œä¹Ÿä¸éœ€è¦yarnç®¡ç†ï¼Œä¸ç”¨é…
	export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.6/etc/hadoop

	# è®¾ç½®Masterçš„ä¸»æœºå
	export SPARK_MASTER_HOST=single
	# æäº¤Applicationçš„ç«¯å£ï¼Œé»˜è®¤å°±æ˜¯è¿™ä¸ªï¼Œä¸‡ä¸€è¦æ”¹å‘¢ï¼Œæ”¹è¿™é‡Œ
	export SPARK_MASTER_PORT=7077
	# æ¯ä¸€ä¸ªWorkeræœ€å¤šå¯ä»¥ä½¿ç”¨çš„cpu coreçš„ä¸ªæ•°ï¼Œæˆ‘è™šæ‹Ÿæœºå°±ä¸€ä¸ª...
	# çœŸå®æœåŠ¡å™¨å¦‚æœæœ‰32ä¸ªï¼Œä½ å¯ä»¥è®¾ç½®ä¸º32ä¸ª
	export SPARK_WORKER_CORES=1
	# æ¯ä¸€ä¸ªWorkeræœ€å¤šå¯ä»¥ä½¿ç”¨çš„å†…å­˜ï¼Œæˆ‘çš„è™šæ‹Ÿæœºå°±2g
	# çœŸå®æœåŠ¡å™¨å¦‚æœæœ‰128Gï¼Œä½ å¯ä»¥è®¾ç½®ä¸º100G
	export SPARK_WORKER_MEMORY=1g

spark-sql
	bin/spark-shell bin/spark-shell --jars /usr/local/mysql/mysql-test/jdbc/mysql-connector-java-5.1.44.jar 
	val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/mysql").option("dbtable", "user").option("user", "root").option("password", "liyuff").load()
	#jdbcDF.show()
	jdbcDF.registerTempTable("test")
	jdbcDF.sqlContext.sql("select user from test").collect.foreach(println)
<<<<<<< HEAD
	jdbcDF.toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	jdbcDF.sqlContext.sql("select user,host from test").toJavaRDD.coalesce(1).saveAsTextFile("temppath")

sqoop
	#Á¬½Ó²é¿´mysql
	bin/sqoop list-tables --username root --password 'liyuff' --connect jdbc:mysql://localhost:3306/mysql?characterEncoding=UTF-8
	#´´½¨hive±í
	bin/sqoop create-hive-table --connect jdbc:mysql://localhost:3306/sqoop_hive?characterEncoding=UTF-8 --table test --username root -password 'liyuff' --hive-database sqoop_hive
	#mysqlÊı¾İµ¼Èëhive
	bin/sqoop import --connect jdbc:mysql://localhost:3306/sqoop_hive?characterEncoding=UTF-8 --table test --username root -password  'liyuff' --fields-terminated-by ',' --hive-import --hive-database sqoop_hive  -m  1
	µ¼ÈëÊı¾İÈ«Îªnull£¨´ı½â¾ö£©
=======
	jdbcDF.toJavaRDD.saveAsTextFile("sparkuser") //å†™å…¥hdfsæ–‡ä»¶è‡³sparkuser/part-00000ï¼Œå½“ç„¶sparkè¿æ¥çš„æ˜¯hadoop
>>>>>>> ab9de6b8a182d780db91baff77fcac0cf3e70647
