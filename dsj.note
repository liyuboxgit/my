python
	python #进入python交互窗口，python注释已#开头
	print('hello,world!')
	exit()
	
	#!/usr/bin/env python3 #在linux，文件头加入这一行，就成为可执行文件
	print('The quick brown fox', 'jumps over', 'the lazy dog') 
	
	#python字符串可以用单引号，也可以用双引号。单引号和双引号是等同的。反斜杠\可以转义，r''单引号中的内容会屏蔽转义
	#布尔值True和False，布尔值计算 and or not(注意和java不一样)
	#空值None
	#运算，python中独有的板除//：8//3=2
	
	#编码
	print('中文'.encode('utf-8')) result is: b'\xe4\xb8\xad\xe6\x96\x87' #python3中，同理 print(b'\xe4\xb8\xad\xe6\x96\x87'.decode('UTF-8'))#中文  
	
	#函数,默认参数 可变参数 递归
	1.def my_abs(x):
    if x>=0:
	    return x
    else:
        return -x
	
	2.def fact(n):
    if n==1:
        return 1
    return n * fact(n - 1)
	
	#包、模块
	目录下面置_init_.py文件，就声明这个目录是个包。包下面有xy.py，就声明xy是该包下面的一个模块
	
	#安装和使用第三方模块 mysql
	pip install mysql-connector-python #安装mysql connector
	import mysql.connector
	#mysql connection config item
	'''dict Found at: mysql.connector.constants
	DEFAULT_CONFIGURATION = {
    'database':None, 
    'user':'', 
    'password':'', 
    'host':'127.0.0.1', 
    'port':3306, 
    'unix_socket':None, 
    'use_unicode':True, 
    'charset':'utf8mb4'...}'''
	conn = mysql.connector.connect(user='root', password='liyuff', database='mysql')
	cursor = conn.cursor()
	cursor.execute('select * from user where user = %s', ('root',))
	values = cursor.fetchall() #value list
	conn.close()
	
	#类
	获取对象的信息
	type() 
	#h = Dog()
	isinstance(h, Animal)
	dir()
	getattr() setattr()以及hasattr()
	使用__slots__限制类的属性
	使用@property将方法设置为属性，供外部访问
	定制类
	元类
hadoop 
	单节点安装：
	set hostname
	/usr/hadoop/hadoop2.8.3/etc/hadoop/slaves
	
	set JAVA_HOME
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hadoop-env.sh
	
	set PORT and TMPDIR
	/usr/hadoop/hadoop2.8.3/etc/hadoop/core-site.xml 
	<configuration>
		<!-- 指定HDFS老大（namenode）的通信地址 -->
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://127.0.0.1:9000</value>
		</property>
		<!-- 指定hadoop运行时产生文件的存储路径 -->
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/root/datahadoop/tmp</value>
		</property>
	</configuration>

	
	
	set namenode and datanode dir
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hdfs-site.xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>/root/datahadoop/name</value>
			<description>namenode上存储hdfs名字的物理存储位置 </description>
		</property>
		<property>
			<name>dfs.data.dir</name>
			<value>/root/datahadoop/data</value>
			<description>datanode上数据块的物理存储位置</description>
		</property>
		<!-- 设置hdfs副本数量,默认为3,设置为1 这样每个block只会存在一份 -->
		<property>
			<name>dfs.replication</name>
			<value>1</value>
		</property>
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
			<description>对hdfs上的文件进行读写时,是否检查权限</description>
		</property>
	</configuration>

	set no password login
	ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	chmod 0600 ~/.ssh/authorized_keys
	format namenode
	bin/hdfs namenode -format
	startup dfs 
	sbin/start-dfs.sh 
	browser with: http://192.168.2.5:50070
	create fileholder in hdfs 
	/bin/hdfs dfs -mkdir /test 
	browser get the hileholder
	stop hdfs
	/sbin/stop-dfs.sh
	set yarn
	mapred-site.xml
	<configuration>
		<!-- 启用yarn作为资源管理框架 -->
		<property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		</property>
	</configuration>
	
	set mapreduce_shuffle
	yarn-site.xml
	<configuration>
		<!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 -->
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
	</configuration>

	startup yarn 
	/sbin/start-yarn.sh
	browser with: http://192.168.2.5:8088
	stop yarn
	/sbin/stop-yarn.sh
	
	设置jvm内存：
	set 'export HADOOP_HEAPSIZE=2000' in $HADOOP_HOME/etc/hadoop/hadoop-env.sh
	set in $HADOOP_HOME/etc/hadoop/mapred-site.xml
	<property>
		<name>mapred.child.java.opts</name>
		<value>-Xmx2000m</value>
	</property>

	/root/dev/hadoop-2.7.6/bin/hdfs dfs -du -h README.md //查看文件大小
	/root/dev/hadoop-2.7.6/bin/hdfs dfs -rmr README //删除文件夹
mysql yum install
	rpm -qa|grep -i mysql #检查是否安装过
	yum -y remove mysql-community-client-5.6.38-2.el7.x86_64
	wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpm
	rpm -ivh mysql57-community-release-el7-8.noarch.rpm
	yum -y install mysql-server
	ls /etc/my.cnf
	ls /var/log/var/log/mysqld.log
	ls /usr/lib/systemd/system/mysqld.service
	cat /etc/my.cnf
	service mysqld restart
	grep "password" /var/log/mysqld.log #随机密码：Nm.eSljs#042
	mysql -uroot -p
	登录后修改密码： alter user 'root'@'localhost' identified by 'Root!!2018';
hive-1.1.0（基于hadoop 2.6）
	修改conf/hive-env.xml中的HADOOP_HOME，HIVE_CONF_DIR
	修改conf/hive-site.xml(conf/hive-default.xml.tmplate)中的：
	<configuration>
		<!--一般启动报错，需要加入下面2property-->
		<property>
			<name>system:java.io.tmpdir</name>
			<value>/root/hive/tmpdir</value>
		</property>
		<property>
			<name>system:user.name</name>
			<value>hive</value>
		</property>
		
		<property>
			<name>javax.jdo.option.ConnectionURL</name>
			<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNoExist=true&amp;useSSL=false</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionDriverName</name>
			<value>com.mysql.jdbc.Driver</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionUserName</name>
			<value>hive</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionPassword</name>
			<value>hive</value>
		</property>
	</configuration>
	
	初始化
	$HIVE_HOME/schematool -dbType mysql -initSchema
	
	启动hive
	$HIVE_HOME/hive

	hql:
	CREATE TABLE pokes (foo INT, bar STRING);
	insert into pokes values(3,'三');
	insert into pokes values(4,'四');//1.1.0不支持
	DESCRIBE pokes;
	
	CREATE TABLE `stu3`(
	  `id` int,
	  `name` string,
	  `age` int)
	ROW FORMAT DELIMITED
	  FIELDS TERMINATED BY ',';
	  
	load data local inpath '/root/t' overwrite into table stu3;
	
	t文件内容:
	3,往来,5
	4,来往,5
	
	csv文件（下面这条才能正确加载csv文件,文件中有逗号，用双引号包裹，双引号需要转义）
	create table csv_t1(a string,b string,c string) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	t文件内容
	3,往来,5
	4,abc,5
	6,'abc',7
	9,"你好,世界",9
	
	create table csv_t2(a string,b string,c string) PARTITIONED BY (ds STRING) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	load data local inpath '/root/t' overwrite into table csv_t2 PARTITION (ds='2008-08-18');--文件内容还是上面的一样
	SELECT a.foo FROM csv_t2 a WHERE a.ds='2008-08-15';
	hdfs中目录结构：
	/user/hive/warehouse/csv_t2/ds=2008-08-15/t
	/user/hive/warehouse/csv_t2/ds=2008-08-18/t
	
	使用spark读取hive数据：
	概述，首先要启动hadoop，hive metastore，spark，必要是还需要mysql的驱动jar放入spark的jars目录下
	1，拷贝hive-site.xml到spark/conf
	2，开启hive元数据服务：hive  --service metastore
	3，开启hadoop服务：sh  $HADOOP_HOME/sbin/start-all.sh
	4.开启spark服务：sh $SPARK_HOME/sbin/start-all.sh
	5.进入spark-shell：spark-shell
		val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
		sqlContext.sql("SELECT * FROM csv_t2 where ds='2008-08-15'").collect().foreach(println)
	6.退出:quit
	
	启动hibeserver2和beeline //1.1.0不支持
	$HIVE_HOME/bin/hiveserver2
	$HIVE_HOME/bin/beeline -u jdbc:hive2://
hbase-1.2.0(基于hadoop-2.6)
	1，download tar -xf cd hbase
	2，edit conf/hbase-env.sh JAVA_HOME
	3. edit conf/hbase-site.xml
	<configuration>
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://localhost:9000/hbase</value>
        </property>
	</configuration>
	4. bin/start-hbase.sh
	5. 进入shell
		bin/hbase shell
		
		create 'test', 'cf'
		--添加数据
		put 'test', 'row1', 'cf:a', '大数据'
		--查询列簇
		get 'test','row1','cf'
		--查询列
		get 'test','row1','cf:a'
		--扫描列
		scan 'test',{COLUMN=>'cf'} or scan 'test',{COLUMNS=>'cf:a'}
		--增加列簇
		alter 'test', 'id'
		--删除列簇
		alter 'test', {NAME => 'id', METHOD => 'delete’}
		--删除列
		delete 'test','row1','cf:a'
		--删除行
		deleteall 'test','row1'
		--全表扫描
		scan 'test'
		//如果出现乱码，在python中：print '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')查看
		
		--禁用和删除表
		disable 'test'
		drop 'test'
	6. 退出shell :quit
spark
	download spark in http://spark.apache.org/
	tar -xf spark*.tar
	bin/spark-shell
	in shell>
		spark.read.textFile("README.md").count() //return 205
		sc.parallelize(List("hello,how are you baby, you are right!")).flatMap(_.split(" ")).map(el=>(el,1)).reduceByKey((a,b)=>a+b).collect().foreach(el=>println(el))
		
	1，举例说明map，flatMap以及reduce，reduceByKey的用法。
	 sc.textFile("text").flatMap(s=>s.split(" ")).map(e=>(e,1)).reduceByKey((a,b)=>a+b).collect()
	 sc.parallelize(Array(1,2,3,4,5)).reduce((a,b)=>a+b)
	map是匹配的意思，flatMap是分解的意思，reduce是计算的意思，reduceByKey是按key计算的意思

	2，sortBy和sortByKey的用法
	  sc.parallelize(Array(3,1,5,2,9)).sortBy(e=>e).collect()
	  --the preline 也可以写成 sc.parallelize(Array(3,1,5,2,9)).sortBy(identity).collect()
	  sc.parallelize(Array((1,2),(4,0),(1,3))).sortBy(a=>a._2).collect() 
	  sc.parallelize(Array((1, 6, 3), (2, 3, 3), (1, 1, 2), (1, 3, 5), (2, 1, 2))).sortBy(e=>(e._1,e._2)).collect()
	sortBy是rdd的排序，输入参数是指定的排序字段或元组

	3，filter的用法
	  sc.parallelize(Array(1,2,3,4,5,6,7,8)).filter(e=>e%2!=0).collect

	4，spark读取文件textFile，输出文件saveAsTextFile([String])
apark-config
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/slaves
	single
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/spark-env.sh
	export JAVA_HOME=/usr/local/jdk
	# 一般来说，spark任务有很大可能性需要去HDFS上读取文件，所以配置上
	# 如果说你的spark就读取本地文件，也不需要yarn管理，不用配
	export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.6/etc/hadoop

	# 设置Master的主机名
	export SPARK_MASTER_HOST=single
	# 提交Application的端口，默认就是这个，万一要改呢，改这里
	export SPARK_MASTER_PORT=7077
	# 每一个Worker最多可以使用的cpu core的个数，我虚拟机就一个...
	# 真实服务器如果有32个，你可以设置为32个
	export SPARK_WORKER_CORES=1
	# 每一个Worker最多可以使用的内存，我的虚拟机就2g
	# 真实服务器如果有128G，你可以设置为100G
	export SPARK_WORKER_MEMORY=1g

spark-sql
	bin/spark-shell bin/spark-shell --jars /usr/local/mysql/mysql-test/jdbc/mysql-connector-java-5.1.44.jar 
	val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/mysql").option("dbtable", "user").option("user", "root").option("password", "liyuff").load()
	#jdbcDF.show()
	jdbcDF.registerTempTable("test")
	jdbcDF.sqlContext.sql("select user from test").collect.foreach(println)
	jdbcDF.toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	jdbcDF.sqlContext.sql("select user,host from test").toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	
	jdbcDF.toJavaRDD.saveAsTextFile("sparkuser") //写入hdfs文件至sparkuser/part-00000，当然spark连接的是hadoop
	
	#json格式
	var df = spark.read.json("person.json")
	df.printSchema
	df.registerTempTable("p")
	spark.sql("select * from p").show()
	spark.sql("select * from p").collect().foreach(print)
	
	联合查询：
	persion.json
	{"id":1,"name":"Tom","age":25}
	"id":2,"name":"LiLi","age":37}
	{"id":3,"name":"Han","age":42}
	addr.json
	{"id":1,"name":"中国","pid":1}
	{"id":2,"name":"美国","pid":1}
	{"id":3,"name":"日本","pid":4}

	spark.read.json("person.json").registerTempTable("p")
	spark.read.json("addr.json").registerTempTable("a")
	spark.sql("select * from p join a on p.id=a.pid").show()
	spark.sql("select * from p left join a on p.id=a.pid").show()
	spark.sql("select * from p right join a on p.id=a.pid").show()
	spark.sql("select * from p full outer join a on p.id=a.pid").show()
	:quit

docker
	1，安装
	uname -r #查看linux内核版本，高于3.10才可以安装docker
	yum update #需要先更新一下
    yum remove docker  docker-common docker-selinux docker-engine #如果安装过，先卸载
    yum -y install yum-utils #为执行下一行命令
    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
    yum list docker-ce --showduplicates | sort -r #查看版本
    yum install docker-ce 安装
    systemctl start docker 启动
    docker version 
	#执行docker version有下面两部分说明安装成功,如报prior storage driver devicemapper failed: Device is Busy，执行yum update xfsprogs
		Client:
		 Version:           18.09.4
		 API version:       1.39
		 Go version:        go1.10.8
		 Git commit:        d14af54266
		 Built:             Wed Mar 27 18:34:51 2019
		 OS/Arch:           linux/amd64
		 Experimental:      false

		Server: Docker Engine - Community
		 Engine:
		  Version:          18.09.4
		  API version:      1.39 (minimum version 1.12)
		  Go version:       go1.10.8
		  Git commit:       d14af54
		  Built:            Wed Mar 27 18:04:46 2019
		  OS/Arch:          linux/amd64
		  Experimental:     false
	2，启动和关闭
	systemctl start docker
	systemctl stop docker
	3，使用mysql(docker mysql)
	docker images #查询本地镜像
	docker pull mysql:5.7 #拉取镜像
	docker run -d -i -p 3306:3306 --name=mysql --restart=no -e MYSQL_ROOT_PASSWORD=liyuff -d mysql:5.7 #启动容器(外部机器连接容器可能需要关闭防火墙)
	docker ps -a #列出所有容器
	docker ps -s #列出成功运行的容器
	docker stop $CONTAINER_ID #停止容器，CONTAINER_ID从上一个命令中获取
	docker rm --force $CONTAINER_ID #移除容器(不能停止的容器要使用forch参数)
		docker rm `docker ps -a -q` #移除所有容器
	docker rmi $image_id #删除镜像(image_id从docker images获取)
	
	通过修改启动脚本配置加速站点
	vim /usr/lib/systemd/system/docker.service 
	ExecStart=/usr/bin/dockerd --registry-mirror=https://3laho3y3.mirror.aliyuncs.com/
	
	docker进入容器，CONTAINER_ID从docker ps中获取
	docker exec -it $CONTAINER_ID /bin/bash
	
	docker双向拷贝
	docker cp $CONTAINER_ID:/xx ./
	
	docker执行java命令
	docker run java java -version
	
	docker文件挂载
	docker run -v "$PWD":/root -w /root java java T //将当前目录挂载到镜像/root目录，并进入该目录，运行java容器，执行java命令java T
	chmod 0777 data/ -R && chmod 0777 logs/ -R //如果挂载出现权限问题，可以如此授权
	docker run -d -v /root/data/:/usr/share/elasticsearch/data -v /root/logs/:/usr/share/elasticsearch/logs -p 9200:9200 --privileged=true elasticsearch:6.5.0 //示例
	
	docker日志跟踪
	docker logs -f 42dq //容器id随机串的前几位
	
	docker安装nginx容器:docker run --name nginx -p 80:80 -p 443:443 -v /data/cloudpivot/program/frontEnd:/usr/share/nginx/html -v /data/cloudpivot/middleware/nginx/conf/nginx.conf:/etc/nginx/nginx.conf --privileged=true -v /data/cloudpivot/middleware/nginx/log:/var/log/nignx -v /data/cloudpivot/middleware/nginx/ssl/:/etc/nginx/ssl/:rw  --restart=always -d nginx:latest
	docker安装redis容器:docker run --name redis -p6379:6379 --restart=always -d redis:4-alpine3.8 --requirepass "H3yuncom"
	docker安装zk容器   :docker run --name zk -p2181:2181 --restart=always -d zookeeper:latest
	docker安装mysql容器:docker run -itd --name mysql -v /data/cloudpivot/middleware/mysql/mysql-data/:/var/lib/mysql -v /data/cloudpivot/middleware/mysql/conf/my.cnf:/etc/mysql/my.cnf --privileged=true -e MYSQL_ROOT_PASSWORD=test123456 -p 3306:3306 --restart=always mysql:5.7
	
	构建镜像
	vi Dockerfile
	From nginx
	RUN echo 'welcome'>/usr/share/nginx/html/index.html
	docker built -t nginx:my .
	
	maven构建镜像
	<plugin>
		<groupId>com.spotify</groupId>
		<artifactId>docker-maven-plugin</artifactId>
		<version>0.4.12</version>
		<configuration>
			<imageName>eureka-server:0.0.1</imageName>
			<baseImage>tomcat</baseImage>
			<entryPoint>["java", "-jar", "/${project.build.finalName}.jar"]</entryPoint>
			<resources>
				<resource>
					<targetPath>/</targetPath>
					<directory>${project.build.directory}</directory>
					<include>${project.build.finalName}.jar</include>
				</resource>
			</resources>
		</configuration>
	</plugin>
	..
	<finalName>eureka-server</finalName>
	mvn clean package docker:build
	docker run -d -p 80:8762 eureka-server:0.0.1
	docker logs --since 30 $CONTAINER_ID
	
	docker-compose编排
	curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
	chmod +x /usr/local/bin/docker-compose
	docker-compose -v

	touch Dockerfile
	vim Dockerfile 

	touch docker-compose.yml
	vi docker-compose.yml 
	/usr/local/apache-maven-3.6.1/bin/mvn clean package
	docker-compose up 
  
	docker-compose编排微服务
	各服务运行maven生成镜像：mvn clean package docker:build
	查看镜像：docker images
	编写docker-compose.yml文件：
	version: '2'
	services:
	  eureka-server:
		image: liyu/eureka-server:0.0.1-SNAPSHOT
		ports:
		  - "8762:8762"

	  eureka-provider:
		image: liyu/eureka-provider:0.0.1-SNAPSHOT
		links:
		  - eureka-server:liyu

	  eureka-consumer:
		image: liyu/eureka-consumer:0.0.1-SNAPSHOT
		ports:
		  - "10002:10002"
		links:
		  - eureka-server:liyu
	运行镜像：docker-compose up
	动态扩容：docker-compose scale eureka-provider=3
	在服务注册首页可见：
	EUREKA-CONSUMER	n/a (1)	(1)	UP (1) - 184463db264e:eureka-consumer:10002
	EUREKA-PROVIDER	n/a (3)	(3)	UP (3) - e6cdf0f76cfe:eureka-provider:10001 , 6a9e4ed2bec8:eureka-provider:10001 , 0e400b56161e:eureka-provider:10001
	
k8s集群安装	
	https://www.kubernetes.org.cn/5462.html	
	注：在所有节点上进行如下操作

	1.设置主机名hostname，管理节点设置主机名为 master 。
	hostnamectl set-hostname master
	需要设置其他主机名称时，可将 master 替换为正确的主机名node1、node2即可。

	2.编辑 /etc/hosts 文件，添加域名解析。
	cat <<EOF >>/etc/hosts
	10.10.10.10 master
	10.10.10.11 node1
	10.10.10.12 node2
	EOF

	3.关闭防火墙、selinux和swap。
	systemctl stop firewalld
	systemctl disable firewalld
	setenforce 0
	sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
	swapoff -a
	sed -i 's/.*swap.*/#&/' /etc/fstab

	4.配置内核参数，将桥接的IPv4流量传递到iptables的链
	cat > /etc/sysctl.d/k8s.conf <<EOF
	net.bridge.bridge-nf-call-ip6tables = 1
	net.bridge.bridge-nf-call-iptables = 1
	EOF
	sysctl --system

	5.配置国内yum源
	yum install -y wget
	mkdir /etc/yum.repos.d/bak && mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak
	wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.cloud.tencent.com/repo/centos7_base.repo
	wget -O /etc/yum.repos.d/epel.repo http://mirrors.cloud.tencent.com/repo/epel-7.repo
	yum clean all && yum makecache
	配置国内Kubernetes源
	cat <<EOF > /etc/yum.repos.d/kubernetes.repo
	[kubernetes]
	name=Kubernetes
	baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
	enabled=1
	gpgcheck=1
	repo_gpgcheck=1
	gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
	EOF
	配置 docker 源
	wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo

	三、软件安装

	注：在所有节点上进行如下操作

	1.安装docker(安装此版本的docker可能会有问题，安装本文件上面的方式安装docker)
	yum install -y docker-ce-18.06.1.ce-3.el7
	systemctl enable docker && systemctl start docker
	docker –version
	Docker version 18.06.1-ce, build e68fc7a
	docker服务为容器运行提供计算资源，是所有容器运行的基本平台。

	2.安装kubeadm、kubelet、kubectl
	yum install -y kubelet kubeadm kubectl
	systemctl enable kubelet
	Kubelet负责与其他节点集群通信，并进行本节点Pod和容器生命周期的管理。
	Kubeadm是Kubernetes的自动化部署工具，降低了部署难度，提高效率。
	Kubectl是Kubernetes集群管理工具。

	四、部署master 节点

	注：在master节点上进行如下操作

	1.在master进行Kubernetes集群初始化(定义ubernetes-version=1.14.2可能会安装失败，去掉就ok了)。
	kubeadm init --kubernetes-version=1.14.2 \
	--apiserver-advertise-address=10.10.10.10 \
	--image-repository registry.aliyuncs.com/google_containers \
	--service-cidr=10.1.0.0/16 \
	--pod-network-cidr=10.244.0.0/16
	定义POD的网段为: 10.244.0.0/16， api server地址就是master本机IP地址。
	这一步很关键，由于kubeadm 默认从官网k8s.grc.io下载所需镜像，国内无法访问，因此需要通过–image-repository指定阿里云镜像仓库地址，很多新手初次部署都卡在此环节无法进行后续配置。
	集群初始化成功后返回如下信息：
	记录生成的最后部分内容，此内容需要在其它节点加入Kubernetes集群时执行。

	kubeadm join 10.10.10.10:6443 --token kekvgu.nw1n76h84f4camj6 \

	--discovery-token-ca-cert-hash sha256:4ee74205227c78ca62f2d641635afa4d50e6634acfaa8291f28582c7e3b0e30e

	2.配置kubectl工具

	mkdir -p /root/.kube
	cp /etc/kubernetes/admin.conf /root/.kube/config
	kubectl get nodes
	kubectl get cs

	3.部署flannel网络
	kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml

	五、部署node节点

	注：在所有node节点上进行如下操作
	执行如下命令，使所有node节点加入Kubernetes集群
	kubeadm join 10.10.10.10:6443 --token kekvgu.nw1n76h84f4camj6 \

	--discovery-token-ca-cert-hash sha256:4ee74205227c78ca62f2d641635afa4d50e6634acfaa8291f28582c7e3b0e30e

	此命令为集群初始化时（kubeadm init）返回结果中的内容。

	六、集群状态检测

	注：在master节点上进行如下操作
	1.在master节点输入命令检查集群状态，返回如下结果则集群状态正常。
	kubectl get nodes
	NAME     STATUS   ROLES    AGE     VERSION
	master   Ready    master   26m     v1.14.2
	node1    Ready    <none>   3m10s   v1.14.2
	node2    Ready    <none>   3m      v1.14.2

	重点查看STATUS内容为Ready时，则说明集群状态正常。

	2.创建Pod以验证集群是否正常。

	kubectl create deployment nginx --image=nginx
	kubectl expose deployment nginx --port=80 --type=NodePort
	kubectl get pod,svc

	七、部署Dashboard

	注：在master节点上进行如下操作

	1.创建Dashboard的yaml文件
	wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
	sed -i 's/k8s.gcr.io/loveone/g' kubernetes-dashboard.yaml
	sed -i '/targetPort:/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort' kubernetes-dashboard.yaml

	2.部署Dashboard
	kubectl create -f kubernetes-dashboard.yaml

	3.创建完成后，检查相关服务运行状态
	kubectl get deployment kubernetes-dashboard -n kube-system
	kubectl get pods -n kube-system -o wide
	kubectl get services -n kube-system
	netstat -ntlp|grep 30001

	4.在Firefox浏览器输入Dashboard访问地址：https://10.10.10.10:30001

	5.查看访问Dashboard的认证令牌

	kubectl create serviceaccount  dashboard-admin -n kube-system
	kubectl create clusterrolebinding  dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin
	kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk '/dashboard-admin/{print $1}')

	单机版上命令：https://blog.csdn.net/u013355826/article/details/82801482	
	journalctl -f -u kubelet
	swapoff -a	
	systemctl status|enable|start kubelet	
	systemctl status|enable|start docker

	kubectl get cs
	kubectl get service
	kubectl get pod

helm安装
	cd /usr/local
	wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz 
	tar -zxvf helm-v2.14.3-linux-amd64.tar.gz
	mv linux-amd64/helm /usr/local/bin/helm
	helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
时间同步
	timedatectl set-timezone Asia/Shanghai
	yum install -y ntp
	ntpdate -u cn.pool.ntp.org
	
普罗米修斯监控：https://www.cnblogs.com/afterdawn/p/9020129.html
	in /usr/local install
	  node_exporter
	  prometheus
	start
	  ./node_exporter &
	  ./prometheus &
	ui
	  http://192.168.43.58:9100/metrics
	  http://192.168.43.58:9090

docker私服
	//拉取镜像
	docker pull docker.io/registry
	//启动容器
	docker run -d -p 5000:5000 --name=registry --restart=always \
		--privileged=true \
		--log-driver=none \
		-v /root/registry/registrydata:/var/lib/registry \
		registry

	//拉取jdk镜像
	docker pull openjdk
	docker run openjdk java -version
	
	//打标
	docker tag openjdk:latest localhost:5000/java:open12.0.2
	//上传镜像到私服
	docker push localhost:5000/java:open12.0.2
	//删除local镜像 
	docker rmi localhost:5000/java:open12.0.2 -f
	//拉取镜像from私服
	docker pull localhost:5000/java:open12.0.2

大数据
apache Mahout
自动化运维
Saltstack
