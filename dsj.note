python
	python #进入python交互窗口，python注释已#开头
	print('hello,world!')
	exit()
	
	#!/usr/bin/env python3 #在linux，文件头加入这一行，就成为可执行文件
	print('The quick brown fox', 'jumps over', 'the lazy dog') 
	
	#python字符串可以用单引号，也可以用双引号。单引号和双引号是等同的。反斜杠\可以转义，r''单引号中的内容会屏蔽转义
	#布尔值True和False，布尔值计算 and or not(注意和java不一样)
	#空值None
	#运算，python中独有的板除//：8//3=2
	
	#编码
	print('中文'.encode('utf-8')) result is: b'\xe4\xb8\xad\xe6\x96\x87' #python3中，同理 print(b'\xe4\xb8\xad\xe6\x96\x87'.decode('UTF-8'))#中文  
	
	#函数,默认参数 可变参数 递归
	1.def my_abs(x):
    if x>=0:
	    return x
    else:
        return -x
	
	2.def fact(n):
    if n==1:
        return 1
    return n * fact(n - 1)
	
	#包、模块
	目录下面置_init_.py文件，就声明这个目录是个包。包下面有xy.py，就声明xy是该包下面的一个模块
	
	#安装和使用第三方模块 mysql
	pip install mysql-connector-python #安装mysql connector
	import mysql.connector
	#mysql connection config item
	'''dict Found at: mysql.connector.constants
	DEFAULT_CONFIGURATION = {
    'database':None, 
    'user':'', 
    'password':'', 
    'host':'127.0.0.1', 
    'port':3306, 
    'unix_socket':None, 
    'use_unicode':True, 
    'charset':'utf8mb4'...}'''
	conn = mysql.connector.connect(user='root', password='liyuff', database='mysql')
	cursor = conn.cursor()
	cursor.execute('select * from user where user = %s', ('root',))
	values = cursor.fetchall() #value list
	conn.close()
	
	#类
	获取对象的信息
	type() 
	#h = Dog()
	isinstance(h, Animal)
	dir()
	getattr() setattr()以及hasattr()
	使用__slots__限制类的属性
	使用@property将方法设置为属性，供外部访问
	定制类
	元类
hadoop 
	单节点安装：
	set hostname
	/usr/hadoop/hadoop2.8.3/etc/hadoop/slaves
	
	set JAVA_HOME
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hadoop-env.sh
	
	set PORT and TMPDIR
	/usr/hadoop/hadoop2.8.3/etc/hadoop/core-site.xml 
	<configuration>
		<!-- 指定HDFS老大（namenode）的通信地址 -->
		<property>
			<name>fs.defaultFS</name>
			<value>hdfs://127.0.0.1:9000</value>
		</property>
		<!-- 指定hadoop运行时产生文件的存储路径 -->
		<property>
			<name>hadoop.tmp.dir</name>
			<value>/root/datahadoop/tmp</value>
		</property>
	</configuration>

	
	
	set namenode and datanode dir
	/usr/hadoop/hadoop2.8.3/etc/hadoop/hdfs-site.xml
	<configuration>
		<property>
			<name>dfs.name.dir</name>
			<value>/root/datahadoop/name</value>
			<description>namenode上存储hdfs名字的物理存储位置 </description>
		</property>
		<property>
			<name>dfs.data.dir</name>
			<value>/root/datahadoop/data</value>
			<description>datanode上数据块的物理存储位置</description>
		</property>
		<!-- 设置hdfs副本数量,默认为3,设置为1 这样每个block只会存在一份 -->
		<property>
			<name>dfs.replication</name>
			<value>1</value>
		</property>
		<property>
			<name>dfs.permissions</name>
			<value>false</value>
			<description>对hdfs上的文件进行读写时,是否检查权限</description>
		</property>
	</configuration>

	set no password login
	ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
	cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
	chmod 0600 ~/.ssh/authorized_keys
	format namenode
	bin/hdfs namenode -format
	startup dfs 
	sbin/start-dfs.sh 
	browser with: http://192.168.2.5:50070
	create fileholder in hdfs 
	/bin/hdfs dfs -mkdir /test 
	browser get the hileholder
	stop hdfs
	/sbin/stop-dfs.sh
	set yarn
	mapred-site.xml
	<configuration>
		<!-- 启用yarn作为资源管理框架 -->
		<property>
			<name>mapreduce.framework.name</name>
			<value>yarn</value>
		</property>
	</configuration>
	
	set mapreduce_shuffle
	yarn-site.xml
	<configuration>
		<!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 -->
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
	</configuration>

	startup yarn 
	/sbin/start-yarn.sh
	browser with: http://192.168.2.5:8088
	stop yarn
	/sbin/stop-yarn.sh
	
	设置jvm内存：
	set 'export HADOOP_HEAPSIZE=2000' in $HADOOP_HOME/etc/hadoop/hadoop-env.sh
	set in $HADOOP_HOME/etc/hadoop/mapred-site.xml
	<property>
		<name>mapred.child.java.opts</name>
		<value>-Xmx2000m</value>
	</property>

	/root/dev/hadoop-2.7.6/bin/hdfs dfs -du -h README.md //查看文件大小
	/root/dev/hadoop-2.7.6/bin/hdfs dfs -rmr README //删除文件夹
mysql yum install
	rpm -qa|grep -i mysql #检查是否安装过
	yum -y remove mysql-community-client-5.6.38-2.el7.x86_64
	wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpm
	rpm -ivh mysql57-community-release-el7-8.noarch.rpm
	yum -y install mysql-server
	ls /etc/my.cnf
	ls /var/log/var/log/mysqld.log
	ls /usr/lib/systemd/system/mysqld.service
	cat /etc/my.cnf
	service mysqld restart
	grep "password" /var/log/mysqld.log #随机密码：Nm.eSljs#042
	mysql -uroot -p
	登录后修改密码： alter user 'root'@'localhost' identified by 'Root!!2018';
hive-1.1.0（基于hadoop 2.6）
	修改conf/hive-env.xml中的HADOOP_HOME，HIVE_CONF_DIR
	修改conf/hive-site.xml(conf/hive-default.xml.tmplate)中的：
	<configuration>
		<!--一般启动报错，需要加入下面2property-->
		<property>
			<name>system:java.io.tmpdir</name>
			<value>/root/hive/tmpdir</value>
		</property>
		<property>
			<name>system:user.name</name>
			<value>hive</value>
		</property>
		
		<property>
			<name>javax.jdo.option.ConnectionURL</name>
			<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNoExist=true&amp;useSSL=false</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionDriverName</name>
			<value>com.mysql.jdbc.Driver</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionUserName</name>
			<value>hive</value>
		</property>
		<property>
			<name>javax.jdo.option.ConnectionPassword</name>
			<value>hive</value>
		</property>
	</configuration>
	
	初始化
	$HIVE_HOME/schematool -dbType mysql -initSchema
	
	启动hive
	$HIVE_HOME/hive

	hql:
	CREATE TABLE pokes (foo INT, bar STRING);
	insert into pokes values(3,'三');
	insert into pokes values(4,'四');//1.1.0不支持
	DESCRIBE pokes;
	
	CREATE TABLE `stu3`(
	  `id` int,
	  `name` string,
	  `age` int)
	ROW FORMAT DELIMITED
	  FIELDS TERMINATED BY ',';
	  
	load data local inpath '/root/t' overwrite into table stu3;
	
	t文件内容:
	3,往来,5
	4,来往,5
	
	csv文件（下面这条才能正确加载csv文件,文件中有逗号，用双引号包裹，双引号需要转义）
	create table csv_t1(a string,b string,c string) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	t文件内容
	3,往来,5
	4,abc,5
	6,'abc',7
	9,"你好,世界",9
	
	create table csv_t2(a string,b string,c string) PARTITIONED BY (ds STRING) row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ("separatorChar"=",") stored as textfile;
	load data local inpath '/root/t' overwrite into table csv_t2 PARTITION (ds='2008-08-18');--文件内容还是上面的一样
	SELECT a.foo FROM csv_t2 a WHERE a.ds='2008-08-15';
	hdfs中目录结构：
	/user/hive/warehouse/csv_t2/ds=2008-08-15/t
	/user/hive/warehouse/csv_t2/ds=2008-08-18/t
	
	使用spark读取hive数据：
	概述，首先要启动hadoop，hive metastore，spark，必要是还需要mysql的驱动jar放入spark的jars目录下
	1，拷贝hive-site.xml到spark/conf
	2，开启hive元数据服务：hive  --service metastore
	3，开启hadoop服务：sh  $HADOOP_HOME/sbin/start-all.sh
	4.开启spark服务：sh $SPARK_HOME/sbin/start-all.sh
	5.进入spark-shell：spark-shell
		val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
		sqlContext.sql("SELECT * FROM csv_t2 where ds='2008-08-15'").collect().foreach(println)
	6.退出:quit
	
	启动hibeserver2和beeline //1.1.0不支持
	$HIVE_HOME/bin/hiveserver2
	$HIVE_HOME/bin/beeline -u jdbc:hive2://
hbase-1.2.0(基于hadoop-2.6)
	1，download tar -xf cd hbase
	2，edit conf/hbase-env.sh JAVA_HOME
	3. edit conf/hbase-site.xml
	<configuration>
        <property>
                <name>hbase.cluster.distributed</name>
                <value>true</value>
        </property>
        <property>
                <name>hbase.rootdir</name>
                <value>hdfs://localhost:9000/hbase</value>
        </property>
	</configuration>
	4. bin/start-hbase.sh
	5. 进入shell
		bin/hbase shell
		
		create 'test', 'cf'
		--添加数据
		put 'test', 'row1', 'cf:a', '大数据'
		--查询列簇
		get 'test','row1','cf'
		--查询列
		get 'test','row1','cf:a'
		--扫描列
		scan 'test',{COLUMN=>'cf'} or scan 'test',{COLUMNS=>'cf:a'}
		--增加列簇
		alter 'test', 'id'
		--删除列簇
		alter 'test', {NAME => 'id', METHOD => 'delete’}
		--删除列
		delete 'test','row1','cf:a'
		--删除行
		deleteall 'test','row1'
		--全表扫描
		scan 'test'
		//如果出现乱码，在python中：print '\xe4\xb8\xad\xe6\x96\x87'.decode('utf-8')查看
		
		--禁用和删除表
		disable 'test'
		drop 'test'
	6. 退出shell :quit
spark
	download spark in http://spark.apache.org/
	tar -xf spark*.tar
	bin/spark-shell
	in shell>
		spark.read.textFile("README.md").count() //return 205
		sc.parallelize(List("hello,how are you baby, you are right!")).flatMap(_.split(" ")).map(el=>(el,1)).reduceByKey((a,b)=>a+b).collect().foreach(el=>println(el))
		
	1，举例说明map，flatMap以及reduce，reduceByKey的用法。
	 sc.textFile("text").flatMap(s=>s.split(" ")).map(e=>(e,1)).reduceByKey((a,b)=>a+b).collect()
	 sc.parallelize(Array(1,2,3,4,5)).reduce((a,b)=>a+b)
	map是匹配的意思，flatMap是分解的意思，reduce是计算的意思，reduceByKey是按key计算的意思

	2，sortBy和sortByKey的用法
	  sc.parallelize(Array(3,1,5,2,9)).sortBy(e=>e).collect()
	  --the preline 也可以写成 sc.parallelize(Array(3,1,5,2,9)).sortBy(identity).collect()
	  sc.parallelize(Array((1,2),(4,0),(1,3))).sortBy(a=>a._2).collect() 
	  sc.parallelize(Array((1, 6, 3), (2, 3, 3), (1, 1, 2), (1, 3, 5), (2, 1, 2))).sortBy(e=>(e._1,e._2)).collect()
	sortBy是rdd的排序，输入参数是指定的排序字段或元组

	3，filter的用法
	  sc.parallelize(Array(1,2,3,4,5,6,7,8)).filter(e=>e%2!=0).collect

	4，spark读取文件textFile，输出文件saveAsTextFile([String])
apark-config
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/slaves
	single
	[root@single spark-2.1.0-bin-hadoop2.7]# cat conf/spark-env.sh
	export JAVA_HOME=/usr/local/jdk
	# 一般来说，spark任务有很大可能性需要去HDFS上读取文件，所以配置上
	# 如果说你的spark就读取本地文件，也不需要yarn管理，不用配
	export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.6/etc/hadoop

	# 设置Master的主机名
	export SPARK_MASTER_HOST=single
	# 提交Application的端口，默认就是这个，万一要改呢，改这里
	export SPARK_MASTER_PORT=7077
	# 每一个Worker最多可以使用的cpu core的个数，我虚拟机就一个...
	# 真实服务器如果有32个，你可以设置为32个
	export SPARK_WORKER_CORES=1
	# 每一个Worker最多可以使用的内存，我的虚拟机就2g
	# 真实服务器如果有128G，你可以设置为100G
	export SPARK_WORKER_MEMORY=1g

spark-sql
	bin/spark-shell bin/spark-shell --jars /usr/local/mysql/mysql-test/jdbc/mysql-connector-java-5.1.44.jar 
	val jdbcDF = spark.read.format("jdbc").option("url", "jdbc:mysql://localhost:3306/mysql").option("dbtable", "user").option("user", "root").option("password", "liyuff").load()
	#jdbcDF.show()
	jdbcDF.registerTempTable("test")
	jdbcDF.sqlContext.sql("select user from test").collect.foreach(println)
	jdbcDF.toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	jdbcDF.sqlContext.sql("select user,host from test").toJavaRDD.coalesce(1).saveAsTextFile("temppath")
	
	jdbcDF.toJavaRDD.saveAsTextFile("sparkuser") //写入hdfs文件至sparkuser/part-00000，当然spark连接的是hadoop
	
	#json格式
	var df = spark.read.json("person.json")
	df.printSchema
	df.registerTempTable("p")
	spark.sql("select * from p").show()
	spark.sql("select * from p").collect().foreach(print)
	
	联合查询：
	persion.json
	{"id":1,"name":"Tom","age":25}
	"id":2,"name":"LiLi","age":37}
	{"id":3,"name":"Han","age":42}
	addr.json
	{"id":1,"name":"中国","pid":1}
	{"id":2,"name":"美国","pid":1}
	{"id":3,"name":"日本","pid":4}

	spark.read.json("person.json").registerTempTable("p")
	spark.read.json("addr.json").registerTempTable("a")
	spark.sql("select * from p join a on p.id=a.pid").show()
	spark.sql("select * from p left join a on p.id=a.pid").show()
	spark.sql("select * from p right join a on p.id=a.pid").show()
	spark.sql("select * from p full outer join a on p.id=a.pid").show()
	:quit

docker
	1，安装
	uname -r #查看linux内核版本，高于3.10才可以安装docker
	yum update #需要先更新一下
    yum remove docker  docker-common docker-selinux docker-engine #如果安装过，先卸载
    yum -y install yum-utils #为执行下一行命令
    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
    yum list docker-ce --showduplicates | sort -r #查看版本
    yum install docker-ce 安装
    systemctl start docker 启动
    docker version 
	#执行docker version有下面两部分说明安装成功
		Client:
		 Version:           18.09.4
		 API version:       1.39
		 Go version:        go1.10.8
		 Git commit:        d14af54266
		 Built:             Wed Mar 27 18:34:51 2019
		 OS/Arch:           linux/amd64
		 Experimental:      false

		Server: Docker Engine - Community
		 Engine:
		  Version:          18.09.4
		  API version:      1.39 (minimum version 1.12)
		  Go version:       go1.10.8
		  Git commit:       d14af54
		  Built:            Wed Mar 27 18:04:46 2019
		  OS/Arch:          linux/amd64
		  Experimental:     false
	2，启动和关闭
	systemctl start docker
	systemctl stop docker
	3，使用mysql(docker mysql)
	docker images #查询本地镜像
	docker pull mysql:5.7 #拉取镜像
	docker run -d -i -p 3306:3306 --name=mysql --restart=no -e MYSQL_ROOT_PASSWORD=liyuff -d mysql:5.7 #启动容器(外部机器连接容器可能需要关闭防火墙)
	docker ps -a #列出所有容器
	docker ps -s #列出成功运行的容器
	docker stop $CONTAINER_ID #停止容器，CONTAINER_ID从上一个命令中获取
	docker rm --force $CONTAINER_ID #移除容器(不能停止的容器要使用forch参数)
	docker rmi $image_id #删除镜像(image_id从docker images获取)
	
	通过修改启动脚本配置加速站点
	vim /usr/lib/systemd/system/docker.service 
	ExecStart=/usr/bin/dockerd --registry-mirror=https://3laho3y3.mirror.aliyuncs.com/
	
	docker进入容器，CONTAINER_ID从docker ps中获取
	docker exec -it $CONTAINER_ID /bin/bash
	
	docker双向拷贝
	docker cp $CONTAINER_ID:/xx ./
	
	构建镜像
	vi Dockerfile
	From nginx
	RUN echo 'welcome'>/usr/share/nginx/html/index.html
	docker built -t nginx:my .
	
	maven构建镜像
	<plugin>
		<groupId>com.spotify</groupId>
		<artifactId>docker-maven-plugin</artifactId>
		<version>0.4.12</version>
		<configuration>
			<imageName>eureka-server:0.0.1</imageName>
			<baseImage>tomcat</baseImage>
			<entryPoint>["java", "-jar", "/${project.build.finalName}.jar"]</entryPoint>
			<resources>
				<resource>
					<targetPath>/</targetPath>
					<directory>${project.build.directory}</directory>
					<include>${project.build.finalName}.jar</include>
				</resource>
			</resources>
		</configuration>
	</plugin>
	..
	<finalName>eureka-server</finalName>
	mvn clean package docker:build
	docker run -d -p 80:8762 eureka-server:0.0.1
	docker logs --since 30 $CONTAINER_ID
	
	docker-compose编排
	curl -L https://github.com/docker/compose/releases/download/1.21.2/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
	chmod +x /usr/local/bin/docker-compose
	docker-compose -v

	touch Dockerfile
	vim Dockerfile 

	touch docker-compose.yml
	vi docker-compose.yml 
	/usr/local/apache-maven-3.6.1/bin/mvn clean package
	docker-compose up 
  
	docker-compose编排微服务
	各服务运行maven生成镜像：mvn clean package docker:build
	查看镜像：docker images
	编写docker-compose.yml文件：
	version: '2'
	services:
	  eureka-server:
		image: liyu/eureka-server:0.0.1-SNAPSHOT
		ports:
		  - "8762:8762"

	  eureka-provider:
		image: liyu/eureka-provider:0.0.1-SNAPSHOT
		links:
		  - eureka-server:liyu

	  eureka-consumer:
		image: liyu/eureka-consumer:0.0.1-SNAPSHOT
		ports:
		  - "10002:10002"
		links:
		  - eureka-server:liyu
	运行镜像：docker-compose up
	动态扩容：docker-compose scale eureka-provider=3
	在服务注册首页可见：
	EUREKA-CONSUMER	n/a (1)	(1)	UP (1) - 184463db264e:eureka-consumer:10002
	EUREKA-PROVIDER	n/a (3)	(3)	UP (3) - e6cdf0f76cfe:eureka-provider:10001 , 6a9e4ed2bec8:eureka-provider:10001 , 0e400b56161e:eureka-provider:10001
